{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a20f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf23db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_process_input', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'attention_dropout', 'bfloat16', 'buffers', 'call_super_init', 'children', 'class_token', 'compile', 'conv_proj', 'cpu', 'cuda', 'double', 'dropout', 'dump_patches', 'encoder', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'heads', 'hidden_dim', 'image_size', 'ipu', 'load_state_dict', 'mlp_dim', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm_layer', 'num_classes', 'parameters', 'patch_size', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'representation_size', 'requires_grad_', 'seq_length', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "Encoder(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (encoder_layer_0): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_1): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_2): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_3): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_4): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_5): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_6): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_7): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_8): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_9): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_10): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_11): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "Sequential(\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n",
      "False\n",
      "LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "# VisionTransformerのインスタンスを作成し、全プロパティを表示\n",
    "import torch\n",
    "import torchvision\n",
    "vit = torchvision.models.vision_transformer.VisionTransformer.\n",
    "vit_instance = vit(image_size=224, patch_size=16, num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072, num_classes=1000)\n",
    "vit_instance.encoder.\n",
    "\n",
    "# # 利用可能な属性一覧を表示\n",
    "# print(dir(vit_instance))\n",
    "# # サブモジュール（例: encoder, heads など）を確認\n",
    "# print(vit_instance.encoder)\n",
    "# print(vit_instance.heads)\n",
    "# # block1のような属性が存在するか確認\n",
    "# print(hasattr(vit_instance, 'block1'))\n",
    "# # 例: encoderのLayerNorm部分だけを取り出す\n",
    "# encoder_ln = vit_instance.encoder.head\n",
    "# print(encoder_ln)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
